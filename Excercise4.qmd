---
title: "Excercise4"
format: html
---

# Excercise 4
By Laura Vetter

## Intro
```{r}
library("readr")
library("dplyr")
library("sf")

wildschwein <- read_delim("wildschwein_BE_2056.csv", ",")

summary(wildschwein)
# Careful! What Timezone is assumed?
sabi <- wildschwein |>
    st_as_sf(coords = c("E", "N"), crs = 2056, remove = FALSE) |> #remove=FALSE is to keep the E & N columns (coords)
    filter(TierName == "Sabi", DatetimeUTC >= "2015-07-01", DatetimeUTC < "2015-07-03")

sabi

```

## Visualize Data
```{r}
library(ggplot2)

ggplot(sabi, aes(E,N, color = DatetimeUTC))+
  geom_point()+
  geom_path()+
  coord_fixed()+
  scale_color_datetime(low="blue", high="red")+
  guides()+
  theme()+
  geom_point()
  
```

## Specify temporal window
In the above dataset, the sampling interval is 15 minutes. If we take a temporal window of 60 minutes, that would mean including 4 fixes. We need to calculate the following Euclidean distances (pos representing single location):

pos[n-2] to pos[n]
pos[n-1] to pos[n]
pos[n] to pos[n+1]
pos[n] to pos[n+2]

## Measure the distance from every point to every other point within this temporal window 

We can use the function distance_by_element from week 2 in combination with lead() and lag() to calculate the Euclidean distance. For example, to create the necessary offset of n-2, we use lag(x, 2). For each offset, we create one individual column.
```{r}
distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

#define temporal window of 60 minutes, partition in 15 minutes steps (sabi data is in 15 min steps)
#1 time behind, two time behind, (lag) one time ahead, two times ahead  (lead)
sabi <- sabi |>
    mutate(
        nMinus2 = distance_by_element(lag(geometry, 2), geometry),  # distance to pos -30 minutes
        nMinus1 = distance_by_element(lag(geometry, 1), geometry),  # distance to pos -15 minutes
        nPlus1  = distance_by_element(geometry, lead(geometry, 1)), # distance to pos +15 mintues
        nPlus2  = distance_by_element(geometry, lead(geometry, 2))  # distance to pos +30 minutes
    )
```
Now we want to calculate the mean distance of nMinus2, nMinus1, nPlus1, nPlus2 for each row. Since we want the mean value per Row, we have to explicitly specify this before mutate() with the function rowwise(). To remove this rowwise-grouping, we end the operation with ungroup().

Note that for the first two positions, we cannot calculate a stepMean since there is no Position n-2 for these positions. This is also true for the last to positions (lacking a position n+2).

```{r}
sabi <- sabi |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()

sabi

```

## Remove Static Points

We can now determine if an animal is moving or not by specifying a threshold distance on stepMean. In our example, we use the mean value as a threshold: Positions with distances below this value are considered static.

```{r}
sabi <- sabi |>
    mutate(static = stepMean < mean(stepMean, na.rm = TRUE))
# here the mean(stepMean)calculates all the stepMeans from Sabi, and then compares the current stepMean to it 

sabi_filter <- sabi |>
    filter(!static)

sabi_filter |>
    ggplot(aes(E, N)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")
sabi_filter



```


# Strava Excercise
## Preparation & Task 1: Segmentation
Import data from strava 
```{r}
library("readr")
library("sf")
library(ggplot2)
library(dplyr)
library("gitcreds")
library(XML) #to read the XML data of "activities/11103101530.gpx" files
library(leaflet) #to show in a map

gpx_parsed <- htmlTreeParse(file = "activities/11239313364.gpx", useInternalNodes = TRUE)

coords <- xpathSApply(doc = gpx_parsed, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed, path = "//trkpt/ele", fun = xmlValue)
timestamps <- xpathSApply(doc = gpx_parsed, path = "//trkpt/time", fun = xmlValue)

df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation),
  timestamp = timestamps
) 
```

### Transformation des CRS

Das GPX file kommt im Degree format im CRS 4326. Es muss umformatiert werden, damit man die Distanzen berechnen kann, daher wollen wir mit st_transform() ins CRS 2056. Dazu müssen wir es zuerst in ein sf umwandeln (dass es eine Spalte mit geom Point hat):

```{r}
new_df <- df %>% 
  st_as_sf(coords=c("lon","lat"), crs=4326, remove=FALSE)

df_transform <- st_transform(new_df, 2056)
df <- df_transform #damit wir nicht so einen komplizierten Namen haben.

# Koordinaten extrahieren
coords <- st_coordinates(df$geometry)
# Koordinaten an Dataframe anfügen
df <- cbind(df, coords)
# Ausgabe
df
```

### erste visualisierung
```{r}
ggplot(df, aes(X,Y))+
  geom_point()+
  geom_path()+
  coord_fixed()+
  scale_color_datetime(low="blue", high="red")+
  guides()+
  theme()+
  geom_point()
```

### define temporal window
first, define the function for the elements later & now (this needs to be defined to compare the measurements in time)
```{r}

  distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}
```
Then, define the frame/window for the step mean. We have running data here, so it makes sense to have a smaller temporal window than 60 minutes. We define a temporal window of 20 secs, partition in 5 sec steps.

```{r}
#1 time behind, two time behind, (lag) one time ahead, two times ahead  (lead)
df <- df |>
    mutate(
        nMinus10 = distance_by_element(lag(geometry, 10), geometry),  # distance to pos -10 seconds
        nMinus5 = distance_by_element(lag(geometry, 5), geometry),  # distance to pos -5 seconds
        nPlus5  = distance_by_element(geometry, lead(geometry, 5)), # distance to pos +5 seconds
        nPlus10  = distance_by_element(geometry, lead(geometry, 10))  # distance to pos +10 seconds
    )
```
now we add the column stepMean to the DF
```{r}
df <- df |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus10, nMinus5, nPlus5, nPlus10))
    ) |>
    ungroup()
```

## remove static points: 
```{r}
# we define the boolean "static" (if we move within a step mean or not)
df <- df |>
    mutate(static = stepMean < mean(stepMean, na.rm = TRUE))

# the mean(stepMean) calculates all the stepMeans, and then compares the current stepMean to it 
df_filter <- df |>
    filter(!static)

```

## Task 2: Specify and apply threshold d & EDA

Die punkte, an denen ich mich in 20 sekunden (step mean) weniger bewegt habe als im gesamten StepMean (gsamtes Stepmean= durchschnitt von allen 4er - Distanzen 

After calculating the Euclidean distances to positions within the temporal window v in task 1, you can explore these values (we stored them in the column stepMean) using summary statistics (histograms, boxplot, summary()): This way we can define a reasonable threshold value to differentiate between stops and moves. There is no “correct” way of doing this, specifying a threshold always depends on data as well as the question that needs to be answered. *In this exercise, use the mean of all stepMean values.*
Store the new information (boolean to differentiate between stops (TRUE) and moves (FALSE)) in a new column named static.
```{r}
# we already did this in the step before and will now compare the hists and boxplots.
#summary()
summary(df)
summary(df_filter)
#here we can see, that thje min stepMean stepMean 13.22, max stepMean 48,27 (m/20 sec)

#boxplot
par(mfrow=c(1,2))
boxplot(df$stepMean)
boxplot(df_filter$stepMean)
```
it is interesting to observe the two boxplots, as the filtered one shows the outliers more clearly as outliers. also, the quantiles are smaller, meaning it is closer to the actual mean speed than the non filtered one. 
ACHTUNG - this is the *stepMean*, not actual velocity in km/h - might be calculated from it in a later step?

```{r}
#histograms
hist(df$stepMean)
hist(df_filter$stepMean)
#here we can see that most of the time I had a StepMean of 20, meaning 20m/20 seconds = 1m/sec=3,6 km/hr? isnt that very low for running? Calculus may not be right.

```

## Task 3: Visualize segmented trajectories
Now visualize the segmented trajectory spatially. Just like last week, you can use ggplot with geom_path(), geom_point() and coord_equal(). Assign colour = static within aes() to distinguish between segments with “movement” and without.
```{r}
# jetzt plotten wir das 
df_omit <- na.omit(df) #damit nicht überall unübersichtliche Lines rum
df_omit |>
    ggplot(aes(X, Y, color=static)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")

df <- df_omit
#looks ok, but I dont understand why there are these lines between the TRUE and NA (after, I ommited them) that make it hard to interpret the graphic.^
```


## Task 4: Segment-based analysis
In applying Laube and Purves (2011), we’ve come as far as step b) in Figure 14.1. In order to complete the last steps (c and d), we need a unique ID for each segment that we can use as a grouping variable. The following function does just that (it assigns unique IDs based on the column static which you created in Task 2). You will learn about functions next week. For now, just copy the following code chunk into your script and run it.

You can use the newly created function rle_id to assign unique IDs to subtrajectories (as shown below). Visualize the moving segments by colourizing them by segment_ID. Then use segment_ID as a grouping variable to determine the segments duration and remove short segments (e.g. segments with a duration < 5 Minutes)

--> I dont understand why we should remove short segments - we only have to remove the ones that say "TRUE", meaning that we stood in the same place, no? Why bc otherwise, if I did a lot of stop & go (e.g. traffic jam) and I remove the trajectory segments where I move less than 5 min in a row, then nothing stays?
```{r}
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}
df <- df |>
    mutate(segment_id = rle_id(static))

df |>
    ggplot(aes(X, Y, color=segment_id)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")

summary(df)

#at some point i wanted segment_id to be numeric, I dont remember why:
#df$segment_id <- as.numeric(df$segment_id)


#define the length of the segments. 
df <- df %>%
  group_by(segment_id) %>%
  mutate(segment_length = n()) %>%
  ungroup()

#This funciton calculates the length of the segment_ids, but does not calculate the TIME of the segment, which is what we actually need, as we dont know wether at some point, a segment was interrupted and took longer. 

library(dplyr)

#therefore, first we need to adapt our time format:
df$timestamp <- as.POSIXct(df$timestamp, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")

# here, we makeuse of the difftime function, with substractimng the beginning and end of the segment, thus calculating the length, and then checking wether it is longer than 1 min. For all segments that are shorter than this, we remove them from the calculus. (however I dont think this makes a lot of sense why we remove tracks that are shorter than one minute)
df <- df %>%
  group_by(segment_id) %>%
  mutate(duration = difftime(max(timestamp), min(timestamp), units = "secs"),
         duration_less_than_60s = as.logical(duration < 60)) %>%
  ungroup()

# now we apply the filter to only show the ones longer than 1 min
df_segments <- df %>%
  filter(!duration_less_than_60s)

df_segments$segment_id <- as.character(df_segments$segment_id)

df_segments |>
    ggplot(aes(X, Y, color=segment_id)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")



#install.packages("OpenStreetMap")
library(OpenStreetMap)

map_tiles <- read_osm(bb(df))

tm_shape(map_tiles) +
  tm_rgb()+
  tm_shape(df_segments) +
  tm_dots(col = "segment_id", size = 0.1, palette = "viridis", shape = 21, border.col = "black", border.lwd = 0.2  )
  
# doenst work bc i dont have JAVA isntalled yet, takes too much time. Next time.
# now it worked
```


# Pedestrian Excercise
## Task 5: Similarity measures (pedestrian)
```{r}
library("readr")
library("dplyr")
library(tidyr)


pedestrian <- read_delim("pedestrian.csv", ",")

#ped <- pedestrian |>
    st_as_sf(coords = c("E", "N"), crs = 2056, remove = FALSE) #remove=FALSE is to keep the E & N columns (coords)
   

ggplot(data = pedestrian, aes(E,N, color = TrajID)) +
  geom_point(color = pedestrian$TrajID) +
  facet_wrap(~TrajID, ncol = 3) 

# I wonder why mine is not rectangular and the ones in the solution are? Is it just a Window-Size problem, i dont think so...


```
## Task 6: Calculate Similarity
### Preparation
```{r}

traj1 <- pedestrian |> 
  filter(TrajID==1) |> 
           subset(select =c(E,N)) |> 
  as.matrix()
  
traj2 <- pedestrian |> 
  filter(TrajID==2) |> 
           subset(select =c(E,N)) |> 
  as.matrix()

traj3 <- pedestrian |> 
  filter(TrajID==3) |> 
           subset(select =c(E,N)) |> 
  as.matrix()

traj4 <- pedestrian |> 
  filter(TrajID==4) |> 
           subset(select =c(E,N)) |> 
  as.matrix()

traj5 <- pedestrian |> 
  filter(TrajID==5) |> 
           subset(select =c(E,N)) |> 
  as.matrix()

traj6 <- pedestrian |> 
  filter(TrajID==6) |> 
           subset(select =c(E,N)) |> 
  as.matrix()

```
or: 
```{r}
trajectory_matrices <- vector("list", length(unique(pedestrian$TrajID)))
names(trajectory_matrices) <- unique(pedestrian$TrajID)

# Iterate over unique TrajIDs and create matrices
for (traj_id in unique(pedestrian$TrajID)) {
  trajectory_matrices[[as.character(traj_id)]] <- pedestrian %>%
    filter(TrajID == traj_id) %>%
    select(E, N) %>%
    as.matrix()
}

# traj1 <- trajectory_matrices[["1"]]


```
Now compare trajectory 1 to trajectories 2-6 using different similarity measures from the package. Your options are. DTW, EditDist, Frechet and LCSS.

### DTW
The dynamic time warping algorithm (DTW) calculates the smallest warp path for the two trajectories. 
```{r}
install.packages("SimilarityMeasures")
library(SimilarityMeasures)

# pointSpacing : An integer value of the maximum index difference between trajectory1 and trajectory2 allowed in the calculation. A negative value sets the point spacing to unlimited.
DTW(traj1, traj2, pointSpacing=-1)
DTW(traj1, traj3, pointSpacing=-1)
DTW(traj1, traj4, pointSpacing=-1)
DTW(traj1, traj5, pointSpacing=-1)
DTW(traj1, traj6, pointSpacing=-1)

#question: what does it mean, maximum index difference, and how is it different to the max distance in Edit Dist?
```
With DTW, you can only compare two trajectories at a time. That is boring.
ChatGPT says: "So while the basic DTW operates on pairs of trajectories, it can be used as a building block or combined with other techniques to enable similarity analysis across multiple trajectories simultaneously." with K means and clustering, it is possible. 
As for similarity measures specifically designed to compare multiple trajectories together, some options mentioned include:
1. Longest Common Subsequence (LCSS) 
2. Edit Distance on Real Sequence (EDR) 
3. Edit Distance with Real Penalty (ERP) 
### automatisation ABORT!!! 
dont use this, it aborts the whole session. But how is it then possible to store the values in the matrix?
```{r}
#install.packages("TraMineR")
library(SimilarityMeasures)


# Assuming you have the six matrices stored as traj1, traj2, ..., traj6
trajectories <- list(traj1, traj2, traj3, traj4, traj5, traj6)

# Create an empty matrix to store the similarity scores
similarity_matrix <- matrix(nrow = length(trajectories), ncol = length(trajectories))
rownames(similarity_matrix) <- paste0("traj", 1:length(trajectories))
colnames(similarity_matrix) <- paste0("traj", 1:length(trajectories))

# Compute the similarity scores using nested loops
for (i in 1:(length(trajectories) - 1)) {
  for (j in (i + 1):length(trajectories)) {
    similarity_score <- EditDist(trajectories[[i]], trajectories[[j]], pointDistance = 20)
    similarity_matrix[i, j] <- similarity_score
    similarity_matrix[j, i] <- similarity_score
  }
}

# Fill the diagonal with 1 (similarity of a trajectory with itself)
diag(similarity_matrix) <- 1

# Print the similarity matrix
print(similarity_matrix)

```

### EditDist
```{r}
# pointDistance: A floating point number representing the maximum distance in each dimension allowed for points to be considered equivalent.
EditDist(traj1, traj2, pointDistance=20)
EditDist(traj1, traj3, pointDistance=20)
EditDist(traj1, traj4, pointDistance=20)
EditDist(traj1, traj5, pointDistance=20)
EditDist(traj1, traj6, pointDistance=20)
EditDist(traj1, traj2, pointDistance=20)

```
### Frechet
```{r}
Frechet(traj1, traj2, testLeash=-1)
Frechet(traj1, traj3, testLeash=-1)
Frechet(traj1, traj4, testLeash=-1)
Frechet(traj1, traj5, testLeash=-1)
Frechet(traj1, traj6, testLeash=-1)
```
### LCSS
LCSStakes very long to compute. The accuracy of the algorithm (pointSpacing = ,pointDistance = and errorMarg =) can be varied to provide faster calculations. Please see Vlachos, Gunopoulos, and Kollios (2002) for more information.

```{r}
LCSS(traj1, traj2, pointSpacing=20, pointDistance=20, errorMarg=5, returnTrans=FALSE)
LCSS(traj1, traj3, pointSpacing=20, pointDistance=20, errorMarg=5, returnTrans=FALSE)
LCSS(traj1, traj4, pointSpacing=20, pointDistance=20, errorMarg=5, returnTrans=FALSE)
LCSS(traj1, traj5, pointSpacing=20, pointDistance=20, errorMarg=5, returnTrans=FALSE)
LCSS(traj1, traj6, pointSpacing=20, pointDistance=20, errorMarg=5, returnTrans=FALSE)

```

Before visualizing your results think about the following: Which two trajectories to you percieve to be most similar, which are most dissimilar? 
We have the comparison of Traj1 with 5 other Trajs. If the value of traj1-Traj2 is lower than Traj1-Traj3 this means that the Traj2 is more similar to Traj1 than Traj3 to Traj1. 
Interestingly, IN DTW and Frechet, Traj2 and Traj6 seem to be the most similar ones, while in LCSS Traj3 seems to be the similar one. 

Now visualize the results from the computed similarity measures. Which measure reflects your own intuition the closest?
```{r}
similarity <- read_delim("similarity_long.csv", ";")

library(ggplot2)
library(tidyr)


ggplot(similarity_long, aes(x = Compare, y = Value, fill=Compare)) +
  geom_col()+
   facet_wrap(~ Type, scales = "free")+
  labs(x = "Compare", y = "Value") 

```
My LCSS looks different than the one in the Ex, as I had a different ErrorMarg and also point distance and spacing. That also changes the pattern quite a lot. 
What do these values actually tell us?







## Other Basemaps 
```{r}
plot(x = df$lon, y = df$lat, type = "l", col = "black", lwd = 3,
     xlab = "Longitude", ylab = "Latitude")

library(leaflet)

leaflet() %>%
  addTiles() %>%
  addPolylines(data = df, lat = ~lat, lng = ~lon, color = "#000000", opacity = 0.8, weight = 3)


get_color <- function(elevation) {
  if (elevation < 500) {
    return("green")
  }
  if (elevation < 1000) {
    return("yellow")
  }
  if (elevation < 1500) {
    return("orange")
  }
  return("red")
}




# New dataset with the new variable for color
df_color <- df %>%
  rowwise() %>%
  mutate(color = get_color(elevation))

df_color$last_color <- dplyr::lag(df_color$color)

# Map
map <- leaflet() %>% addTiles()
for (color in levels(as.factor(df_color$color))) {
  map <- addPolylines(map, lat = ~lat, lng = ~lon, data = df_color[df_color$color == color | df_color$last_color == color, ], color = ~color)
}
map
```


